{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping\n",
    "\n",
    "Note: I have not tested this on the lab computers at all, so none of this might work.\n",
    "\n",
    "*(Adapted from https://realpython.com/beautiful-soup-web-scraper-python)*\n",
    "\n",
    "Web scraping is the act of extracting information from a web page. This can include copy-pasting something by hand, but we usually are referring to an automated process. There are two steps to this: acquiring the structured, unprocessed data from the target web page (usually HTML or XML), and extracting the data into a useful data type (like a DataFrame).\n",
    "\n",
    "Web scraping is [hard](https://realpython.com/beautiful-soup-web-scraper-python/) because every web site is different and web sites constantly change. Any scraper you write may require constant maintenance.\n",
    "\n",
    "In this activity, we'll scrape data from this [Fake Python Jobs](https://realpython.github.io/fake-jobs/) page that was created for web scraping practice. (The terms of service of individual web sites may make it illegal to use web scraping code on them. FYI. Also some sites use settings to block web scrapers and will cause your code to fail.)\n",
    "\n",
    "We'll use the `requests` library to get the unprocessed data from the web page and the `BeautifulSoup` library to parse that data into the information we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Inspect your data source\n",
    "\n",
    "Before you start coding, you should look at the data source. (This isn't limited to web scraping...) Open up [Fake Python Jobs](https://realpython.github.io/fake-jobs/) in your browser.\n",
    "\n",
    "Scroll through the site, see what happens when you click on things. Think about what data is present that someone might want to scrape.\n",
    "\n",
    "Next, explore the HTML behind the scenes. This will give you an idea of how the raw data is structured. Right-click on something of interest and choose 'Inspect'. If that's not an option, Ctrl+Shift+I should open the Developer Tools on your browser. (If that still doesn't work, you can Ctrl+S to save the web page and open it in a text editor.)\n",
    "\n",
    "Now we're ready to download the HTML code. We can do this manually, but we want to automate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download HTML from the web page\n",
    "\n",
    "For a static web page like the Fake Jobs we're looking at, this is super easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "URL = \"https://realpython.github.io/fake-jobs/\"\n",
    "page = requests.get(URL)  # issues an HTTP GET request to URL & stores response as page\n",
    "\n",
    "print(page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like I said, super easy. The value of `page.text` should look like the HTML you looked at in Step 0.\n",
    "\n",
    "This is a bit harder if we want the results of some sort of search query or user input actions, and quite a bit harder if the page requires authentication to get to. (Still possible, but outside the scope of today's lesson.) \n",
    "\n",
    "It's also messy if you're getting data from some dynamic web site with lots of Javascript  that has to execute before you can get the data. You'll have to look for a different library for that (like Selenium). Just know that it's out there if you need it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now you have the HTML. Just read through it carefully and write down all the data you need. Web scraping complete! ðŸ˜Š\n",
    "\n",
    "Not really. But we do need to study the HTML to understand the structure surrounding the data, which will allow us to exploit that structure in the next step.\n",
    "\n",
    "Let's assume we want to extract the data that's on each of the job tiles (job title, company, location, date, and URL to the 'Apply' page). Try to locate one of the job tiles in the HTML above or by using Inspect on the page in your browser.\n",
    "\n",
    "Seriously. Go look. This is probably the hardest part of web scraping, and if the page is well-designed, it's not that hard. When you think you have some idea about the structure, scroll down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 1000px;\">&nbsp;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice `<div class=\"card\">`? To me, that looks like the container for each of the job tiles. I've extracted one for a closer look:\n",
    "\n",
    "```html\n",
    "<div class=\"card\">\n",
    "  <div class=\"card-content\">\n",
    "    <div class=\"media\">\n",
    "      <div class=\"media-left\">\n",
    "        <figure class=\"image is-48x48\">\n",
    "          <img src=\"https://files.realpython.com/media/real-python-logo-thumbnail.7f0db70c2ed2.jpg?__no_cf_polish=1\" alt=\"Real Python Logo\">\n",
    "        </figure>\n",
    "      </div>\n",
    "      <div class=\"media-content\">\n",
    "        <h2 class=\"title is-5\">Senior Python Developer</h2>\n",
    "        <h3 class=\"subtitle is-6 company\">Payne, Roberts and Davis</h3>\n",
    "      </div>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"content\">\n",
    "      <p class=\"location\">\n",
    "        Stewartbury, AA\n",
    "      </p>\n",
    "      <p class=\"is-small has-text-grey\">\n",
    "        <time datetime=\"2021-04-08\">2021-04-08</time>\n",
    "      </p>\n",
    "    </div>\n",
    "    <footer class=\"card-footer\">\n",
    "        <a href=\"https://www.realpython.com\" target=\"_blank\" class=\"card-footer-item\">Learn</a>\n",
    "        <a href=\"https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html\" target=\"_blank\" class=\"card-footer-item\">Apply</a>\n",
    "    </footer>\n",
    "  </div>\n",
    "</div>\n",
    "```\n",
    "\n",
    "To make things a little easier, next I've deleted the elements we don't care about (the image and the Learn link):\n",
    "\n",
    "```html\n",
    "<div class=\"card\">\n",
    "  <div class=\"card-content\">\n",
    "    <div class=\"media\">\n",
    "      \n",
    "      <div class=\"media-content\">\n",
    "        <h2 class=\"title is-5\">Senior Python Developer</h2>\n",
    "        <h3 class=\"subtitle is-6 company\">Payne, Roberts and Davis</h3>\n",
    "      </div>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"content\">\n",
    "      <p class=\"location\">\n",
    "        Stewartbury, AA\n",
    "      </p>\n",
    "      <p class=\"is-small has-text-grey\">\n",
    "        <time datetime=\"2021-04-08\">2021-04-08</time>\n",
    "      </p>\n",
    "    </div>\n",
    "    <footer class=\"card-footer\">\n",
    "        \n",
    "        <a href=\"https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html\" target=\"_blank\" class=\"card-footer-item\">Apply</a>\n",
    "    </footer>\n",
    "  </div>\n",
    "</div>\n",
    "```\n",
    "\n",
    "Now, focus on the data we care about and look for text that may help you identify it. Certain HTML tags and classes are useful here.\n",
    "\n",
    "### Task\n",
    "\n",
    "In the cell below, double-click it, then copy-paste just the lines/HTML tags that contain the data we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answers here\n",
    "```html\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see some structure? Scroll down to continue.\n",
    "\n",
    "<div style=\"height: 500px;\">&nbsp;</div>\n",
    "\n",
    "Here are the five data items we want:\n",
    "\n",
    "```html\n",
    "<h2 class=\"title is-5\">Senior Python Developer</h2>\n",
    "\n",
    "<h3 class=\"subtitle is-6 company\">Payne, Roberts and Davis</h3>\n",
    "\n",
    "<p class=\"location\">\n",
    "        Stewartbury, AA\n",
    "</p>\n",
    "\n",
    "<time datetime=\"2021-04-08\">2021-04-08</time>\n",
    "\n",
    "<a href=\"https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html\" target=\"_blank\" class=\"card-footer-item\">Apply</a>\n",
    "\n",
    "```\n",
    "\n",
    "The `class` value can help us find job title, company, and location. In HTML, an object can have multiple classes that are separated by spaces. Some of those may be uniquely identifying, others may be generic and used for formatting. Based on some experimentation, here's the tags/classes/attributes I think we need to focus on:\n",
    "\n",
    "* *job title* - tag 'h2', class 'title'\n",
    "* *company* - tag 'h3', class 'company'\n",
    "* *location* - tag 'p', class 'location'\n",
    "* *date* - tag 'time'\n",
    "* *apply url* - tag 'a', class ???\n",
    "\n",
    "The apply url is tricky, because while it has a class, that class is not unique. Look back at the full html for the job tile: both the Apply and Learn links use the same class:\n",
    "\n",
    "```html\n",
    "<footer class=\"card-footer\">\n",
    "    <a href=\"https://www.realpython.com\" target=\"_blank\" class=\"card-footer-item\">Learn</a>\n",
    "    <a href=\"https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html\" target=\"_blank\" class=\"card-footer-item\">Apply</a>\n",
    "</footer>\n",
    "```\n",
    "\n",
    "So, will we be able to scrape the apply url? How might we differentiate between the two `<a href>`s in the footer? Think about this.\n",
    "\n",
    "Another fact about our data is that it is all wrapped in a `<div class=\"card\">`. That might come in handy later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's start to setup for extracting data from the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs4 is the module name they went with for BeautifulSoup *shrug*\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# we reuse the page object from the earlier use of requests\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we say `page.content` instead of `page.text` because that can avoid some problems with character encodings (think the UTF-8 mess from the Olympics data).\n",
    "\n",
    "Now, how to extract all the job tiles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_elements = soup.find_all(\"div\", class_=\"card\")  # find_all returns an iterable\n",
    "\n",
    "print(job_elements[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see where this is going?\n",
    "\n",
    "Let's start writing a function to process each job tile.\n",
    "\n",
    "### Task\n",
    "\n",
    "In the function draft below, edit the code for company_element and location_element to use the correct values.\n",
    "\n",
    "Then, add code to extract the date element. Be careful with the `class_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_job_tile(job_element):\n",
    "    \"\"\"Returns dict with desired data elements.\n",
    "    \n",
    "    Args:\n",
    "      job_element (bs4.element.Tag): a single job tile (<div class=\"card\">)\n",
    "      \n",
    "    Returns:\n",
    "      dict: with keys title, company, location, date, and url\n",
    "      \n",
    "    \"\"\"\n",
    "    title_element = job_element.find(\"h2\", class_=\"title\")\n",
    "    company_element = job_element.find(\"h2\", class_=\"title\")\n",
    "    location_element = job_element.find(\"h2\", class_=\"title\")\n",
    "    \n",
    "    print(title_element)\n",
    "    print(company_element)\n",
    "    print(location_element)\n",
    "    print()\n",
    "    \n",
    "    return dict()  # we'll get there eventually\n",
    "\n",
    "# to test our function\n",
    "process_job_tile(job_elements[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Progress! We still need the url, and we want to remove all the HTML tag mess from what we have so far.\n",
    "\n",
    "The latter is easy. The HTML tag object that BeautifulSoup gives us have a `.text` attribute. These are Python strings. When we're working with strings from a foreign data source, there's a chance of extra hidden whitespace (again, think about the Olympics CSV exercise), so the `.strip()` method is a good idea.\n",
    "\n",
    "### Task\n",
    "\n",
    "Modify the function we're working on to instead print `title_element.text.strip()`. Likewise for the other elements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish up, we need to solve that Apply url issue: both the Learn and Apply links have the same tag and class. What ideas did you come up with?\n",
    "\n",
    "I came up with two options:\n",
    "\n",
    "1. the Apply link is the second `<a href>` in the `<footer class=\"card-footer\">`, so we could `.find_all()` on the footer and pick the second one\n",
    "    1. similarily, it is the second `<a href>` with `class=\"card-footer-item\"`\n",
    "2. the Apply link is the only `<a href>` that has \"Apply\" as its `.text`\n",
    "\n",
    "Pick which option you want to try <font color=\"green\">(or, try both!)</font>. \n",
    "\n",
    "Hints: \n",
    "\n",
    "1. if you go with option 1, you can leverage the techniques we've already used: starting with the current job element, `.find()` to get the footer and/or `.find_all()` to get both links\n",
    "2. if you go with option 2, you can [include a](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#the-string-argument) `string=` argument in `.find_all()` to filter by the text (or you can `.find_all()` without filtering, then filter the results manually)\n",
    "\n",
    "Once you have the `<a>` tag you need, you can extract the URL from its `href` attribute. Try `apply_element.href` or `apply_element['href']`.\n",
    "\n",
    "### Task\n",
    "\n",
    "Modify the `process_job_tile()` function to extract the Apply url."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finishing up\n",
    "\n",
    "Nearly there!\n",
    "\n",
    "### Task\n",
    "\n",
    "One last task: fix the function to return a dictionary instead of printing everything, then write a loop to extract the data from all of the job listings.\n",
    "\n",
    "For bonus \"points\", feed the data into a DataFrame/database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Suppose you only wanted a subset of the data. You're applying to be a Python programmer, not just any job. Two options:\n",
    "\n",
    "1. Scrape all the data, then filter locally (e.g., in a DataFrame/database)\n",
    "2. Filter before scraping the data.\n",
    "\n",
    "For 1, you can reference Pandas or SQL documentation.\n",
    "\n",
    "For 2, you can make use of more `.find_all()` filtering. Check [this](https://realpython.com/beautiful-soup-web-scraper-python/#find-elements-by-class-name-and-text-content) out.\n",
    "\n",
    "For a more complete study of BeautifulSoup and other techniques for navigating the web document, read through [this](https://www.dataquest.io/blog/web-scraping-python-using-beautiful-soup/). That can give you some strategies when your raw HTML is not well structured (no `class=` attributes that you can use, e.g.).\n",
    "\n",
    "Happy web scraping!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
